# Data Drift Sentinel

A Streamlit application for monitoring data drift between baseline and current datasets using Population Stability Index (PSI) and other statistical measures.

ğŸ”— **Repository**: [https://github.com/spacialglaciercom-lab/data-drift-sentinel](https://github.com/spacialglaciercom-lab/data-drift-sentinel)

## Features

- **Deterministic Statistics Report**: Comprehensive statistical comparison between baseline and current datasets
- **PSI Calculation**: Population Stability Index for measuring data drift
- **Severity Thresholds**: Configurable thresholds for drift severity classification
- **Pydantic Models**: Structured, serializable `DriftReport` model with schema validation
- **Optional Metrics**: KS test p-values and Jensen-Shannon divergence
- **Optional LLM Summary**: AI-powered summary grounded in computed JSON facts (optional feature)

## Project Structure

```
data-drift-sentinel/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ compute_drift.py       # Main compute_drift function
â”‚   â”œâ”€â”€ models.py              # Pydantic models for DriftReport
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ psi_calculator.py      # PSI calculation implementation
â”‚   â”œâ”€â”€ statistics.py          # Statistical measures and comparisons
â”‚   â”œâ”€â”€ metrics.py             # Additional metrics (KS test, JS divergence)
â”‚   â”œâ”€â”€ report_builder.py      # Build DriftReport from detection results
â”‚   â”œâ”€â”€ llm_summary.py         # Optional LLM-based summary generation
â”‚   â”œâ”€â”€ drift_detector.py      # Legacy DriftDetector class
â”‚   â””â”€â”€ utils.py               # Utility functions
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                # Streamlit main application
â”‚   â”œâ”€â”€ components.py          # UI components
â”‚   â””â”€â”€ utils.py               # Streamlit utilities
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ 1_ğŸ“¤_Upload.py
â”‚   â”œâ”€â”€ 2_ğŸ”_Schema_Quality.py
â”‚   â”œâ”€â”€ 3_ğŸ“Š_Drift_Report.py
â”‚   â”œâ”€â”€ 4_ğŸ¤–_LLM_Summary.py
â”‚   â””â”€â”€ 5_ğŸ’¾_Export.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_psi_synthetic.py
â”‚   â”œâ”€â”€ test_severity_mapping.py
â”‚   â””â”€â”€ test_schema_diff.py
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Installation

```bash
pip install -r requirements.txt
```

## Configuration

### LLM API Key (Optional)

For LLM summary generation, configure your API key using one of these methods:

**Recommended: Streamlit Secrets**
1. Copy `.streamlit/secrets.toml.example` to `.streamlit/secrets.toml`
2. Add your API key: `LLM_API_KEY = "your-api-key-here"`
3. Restart Streamlit

**Alternative: Environment Variable**
- Set `OPENAI_API_KEY` or `LLM_API_KEY` environment variable
- Or use `.env` file (copy `.env.example` to `.env` and add your key)

The app will automatically detect the API key and enable LLM summary features. If no API key is found, you'll see: "LLM summary disabled â€” add API key to Streamlit secrets."

## Usage

Run the Streamlit multipage application:

```bash
streamlit run app/main.py
```

The app will automatically detect the `pages/` directory and create a navigation menu with:
- **ğŸ“¤ Upload** - Upload baseline and current datasets
- **ğŸ” Schema & Quality** - View schema differences and configure drift detection
- **ğŸ“Š Drift Report** - Compute and visualize drift with interactive charts
- **ğŸ¤– LLM Summary** - Generate AI-powered summaries (optional)
- **ğŸ’¾ Export** - Export results as JSON or CSV

## Testing

```bash
pytest tests/
```

## Deployment

See [DEPLOYMENT.md](DEPLOYMENT.md) for detailed instructions on deploying to GitHub.

Quick start:
```bash
git init
git add .
git commit -m "Initial commit: Data Drift Sentinel"
# Create repository on GitHub, then:
git remote add origin https://github.com/YOUR_USERNAME/data-drift-sentinel.git
git branch -M main
git push -u origin main
```

## compute_drift Function

The main `compute_drift` function provides comprehensive drift detection:

```python
from src.compute_drift import compute_drift
from src.config import DriftConfig, SeverityThresholds

# With default config
report = compute_drift(baseline_df, current_df)

# With custom config
config = DriftConfig(
    bins=15,
    min_bins=5,
    max_categories=10,
    include_ks=True,
    include_js=True,
    binning_method="adaptive"
)

thresholds = SeverityThresholds(
    low_threshold=0.1,
    medium_threshold=0.25,
    high_threshold=0.5
)

report = compute_drift(baseline_df, current_df, config, thresholds)
```

**Features:**
- **Numeric columns**: PSI (with robust binning), KS test p-value, JS divergence, missing delta, summary stats deltas
- **Categorical columns**: PSI using top K categories + 'other' bucket, JS divergence, missing delta
- **Robust binning**: Adapts to small sample sizes
- **Schema diff**: Detects added/removed columns and type changes

## DriftReport Model

The `DriftReport` Pydantic model provides a structured, serializable JSON output with:

- **Dataset Metadata**: Row/column counts, common columns
- **Schema Diff**: Added/removed columns, type changes
- **Per-Column Metrics**: PSI, missing delta, severity, optional KS p-value and JS divergence, summary stats deltas
- **Top Changed Columns**: List of columns with highest drift, sorted by PSI

Example usage:

```python
from src.compute_drift import compute_drift
import pandas as pd

# Load your data
baseline_df = pd.read_csv('baseline.csv')
current_df = pd.read_csv('current.csv')

# Compute drift
report = compute_drift(baseline_df, current_df)

# Serialize to JSON (stable, deterministic)
json_output = report.model_dump_json()

# Access structured data
print(report.dataset_metadata.baseline_rows)
print(report.per_column_metrics['age'].psi)
print(report.top_changed_columns[0].column_name)
```

## License

See LICENSE file for details.
